---
title: "lab_05"
author: "Zhuravlev"
date: '15 апреля 2017 г '
output: html_document
---

```{r Данные и пакеты, echo=T }
# загрузка пакетов
library('ISLR')              # набор данных Carseats
library('GGally')            # матричные графики
library('boot')              # расчёт ошибки с кросс-валидацией
str(Carseats)
# константы
my.seed <- 1

car <- Carseats[, c('Sales', 'Price', 'Advertising', 'US', 'ShelveLoc')]
head(car)

# графики разброса
ggpairs(car)

```

```{r Метод проверочной выборки, echo=T}
# общее число наблюдений
n <- nrow(car)

# доля обучающей выборки
train.percent <- 0.5

# выбрать наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(n, n * train.percent)
inTrain

# Модель со всеми объясняющими переменными ##############################################################

# подгонка линейной модели на обучающей выборке
fit.lm.1 <- lm(Sales ~ ., 
               subset = inTrain, data = car)
# считаем MSE на тестовой выборке
mean((car$Sales[-inTrain] - predict(fit.lm.1,
                              car[-inTrain, ]))^2)

# Модель только с непрерывними объясняющими переменными ##########################################################

# подгонка линейной модели на обучающей выборке
fit.lm.2 <- lm(Sales ~ Price + Advertising, 
               subset = inTrain, data = car)
# считаем MSE на тестовой выборке
mean((car$Sales[-inTrain] - predict(fit.lm.2,
                              car[-inTrain, ]))^2)
```

```{r Перекрёстная проверка по отдельным наблюдениям (LOOCV), echo=T}
# подгонка линейной модели на обучающей выборке
fit.glm <- glm(Sales ~ ., data = car)
# считаем LOOCV-ошибку
cv.err <- cv.glm(car, fit.glm)
# результат: первое число -- по формуле LOOCV-ошибки,
#  второе -- с поправкой на смещение
cv.err$delta[1]
  

# подгонка линейной модели на обучающей выборке
fit.glm.2 <- glm(Sales ~ Price + Advertising, data = car)
# считаем LOOCV-ошибку
cv.err.2 <- cv.glm(car, fit.glm.2)
# результат: первое число -- по формуле LOOCV-ошибки,
#  второе -- с поправкой на смещение
cv.err.2$delta[1]

```

```{r k-кратная перекрёстная проверка, echo=T}
# вектор с ошибками по 5-кратной и 10-кратной кросс-валидации для модели со всеми объясняющими переменными
cv.err.k.fold.1.5 <- cv.glm(car, fit.glm,
                             K = 5)$delta[1]
cv.err.k.fold.1.10 <- cv.glm(car, fit.glm,
                        K = 10)$delta[1]
# результат
cv.err.k.fold.1.5
cv.err.k.fold.1.10

# вектор с ошибками по 5-кратной и 10-кратной кросс-валидации для модели только с непрерывными объясняющими переменными
cv.err.k.fold.2.5 <- cv.glm(car, fit.glm.2,
                            K = 5)$delta[1]
cv.err.k.fold.2.10 <- cv.glm(car, fit.glm.2,
                             K = 10)$delta[1]
# результат
cv.err.k.fold.2.5
cv.err.k.fold.2.10

```
По минимуму ошибки лучшая модель - со всеми объясняющими переменными. Все кроссвалидации сходятся на 1ной модели.

```{r Бутстреп, echo=T}
# функция для расчёта коэффициентов ПЛР по выборке из данных для модели со всеми объясняющими переменными
boot.fn <- function(data, index){
  coef(lm(Sales ~ ., data = data, subset = index))
}
boot.fn(car, 1:n)


# функция для расчёта коэффициентов ПЛР по выборке из данных для модели только с непрерывными объясняющими переменными
boot.fn.2 <- function(data, index){
  coef(lm(Sales ~ Price + Advertising, data = data, subset = index))
}
boot.fn.2(car, 1:n)

# применяем функцию boot для вычисления стандартных ошибок параметров
#  (1000 выборок с повторами)
# для модели со всеми объясняющими переменными:
boot(car, boot.fn, 1000)

# для модели только с непрерывными объясняющими переменными:
boot(car, boot.fn.2, 1000)
# сравним с МНК
# для модели со всеми объясняющими переменными:
summary(lm(Sales ~ ., data = car))$coef

# для модели только с непрерывными объясняющими переменными:
summary(lm(Sales ~ Price + Advertising, data = car))$coef

```
Оценки отличаются из-за того, что МНК -- параметрический метод с допущениями